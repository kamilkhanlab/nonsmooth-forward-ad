var documenterSearchIndex = {"docs":
[{"location":"functions.html#Exported-Structs","page":"Exported Functions","title":"Exported Structs","text":"","category":"section"},{"location":"functions.html","page":"Exported Functions","title":"Exported Functions","text":"Main.NonsmoothFwdAD.GeneralizedDiff.AFloat","category":"page"},{"location":"functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.AFloat","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.AFloat","text":"AFloat{val::Float64, dot::Vector{Float64}, ztol::Float64}\n\nType representing generalized derivative information.\n\nFields\n\nval::Float64: output value\ndot::Vector64: output generalized derivative. Set to I.\nztol::Float64: capture radius for nonsmooth operations with kinks. Set to 0.0 by default.\n\nNotes:\n\nOperator overloading macros defining generalized differentiation rules for elemental operations require AFloat inputs. Structs cannot be defined outside main module file. \n\n\n\n\n\n","category":"type"},{"location":"functions.html#Exported-Functions","page":"Exported Functions","title":"Exported Functions","text":"","category":"section"},{"location":"functions.html#Functions-from-GeneralizedDiff.jl","page":"Exported Functions","title":"Functions from GeneralizedDiff.jl","text":"","category":"section"},{"location":"functions.html","page":"Exported Functions","title":"Exported Functions","text":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative\r\nMain.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative\r\nMain.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative\r\nMain.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient\r\nMain.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference","category":"page"},{"location":"functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative","text":"eval_ld_derivative(f::Function, x::Vector{Float64}, xDot::T; ztol::Float64)\n\nCompute the function vector-valued f(x) and the LD-derivative f'(x; xDot).\n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot. \nx::Vector{Float64}: input domain vector \nxDot::T: output generalized derivative. It must have full row rank. Set to I by default.\n\nwhere T is either Matrix{Float64} or Vector{Vector{Float64}}\n\nKeywords\n\nztol::Float64: capture radius for nonsmooth operations with kinks. Set to 0.0 by default.\n\nReturns\n\ny::Vector{Float64}: \nyDot::T where T is either Matrix{Float64} or Vector{Vector{Float64}}: the lexicographic derivative f'(x; xDot) \n\nExample\n\nTo evaluate the LD-derivative of function f on vector x: \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\nxDot = Matrix{Float64}(I(length(x0)))\n_, yDot = eval_ld_derivative(f, x, xDot)\n\n# output \n\nyDot = [0.0, 0.0, -1.0] \n\n\n\n\n\n","category":"function"},{"location":"functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative","text":"eval_dir_derivative (f::Function, x::Vector{Float64}, xDot::Vector{Float64}; ztol::Float64)\n\nCompute the function vector-valued y = f(x) and the directional derivative yDeriv = f'(x; xDot). \n\nSee eval_ld_derivative for more details on function inputs.\n\nReturns\n\ny::Vector{Float64}: function value as a vector\nyDot::Vector{Float64}: directional derivative f'(x; xDot::Vector) as a vector \n\nNotes\n\nFor this function, argument xDot must be of type Vector{Float64}. \n\nExample\n\nTo calculate the directional derivative of function f in the direction of vector x. \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\nxDot = [1.0, 0.0, 1.0]\n_, yDeriv = eval_dir_derivative(f, x, xDot)\n\n# output \n\nyDeriv = [-1.0] \n\n\n\n\n\n","category":"function"},{"location":"functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative","text":"eval_gen_derivative (f::Function, x::Vector{Float64}, xDot::Matrix{Float64}; ztol::Float64)\n\nCompute the function vector-valued y = f(x) and the generalized derivative yDeriv = Df(x; xDot). \n\nSee eval_ld_derivative for more details on function inputs.\n\nReturns\n\ny::Vector{Float64}: function value as a vector\nyDeriv::Matrix{Float64}: lexicographic derivative Df(x; xDot::Vector) as a vector \n\nNotes\n\nFor this function, argument xDot must be of type Matrix{Float64}. \n\nExample\n\nTo calculate the generalized derivative of function f in the direction of vector x. \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\nxDot = Matrix{Float64}(I(length(x0)))\n_, yDeriv = eval_gen_derivative(f, x, xDot)\n    \n# output \n    \nyDeriv = [0.0, 0.0, -1.0]] \n\n\n\n\n\n","category":"function"},{"location":"functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient","text":"eval_gen_gradient (f::Function, x::Vector{Float64}, xDot::Matrix{Float64}; ztol::Float64)\n\nCompute the function vector-valued y = f(x) and the generalized gradient yGrad = yDeriv' for scalar valued functions f.\n\nSee eval_ld_derivative for more details on function inputs.\n\nReturns\n\ny::Vector{Float64}: function value as a vector\nyGrad:Vector{Float64}: directional derivative f'(x; xDot::Vector) as a vector \n\nNotes\n\nFor this function, argument xDot must be of type Matrix{Float64}. \n\nExample\n\nTo calculate the generalized gradient of function f in the direction of vector x. \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\nxDot = Matrix{Float64}(I(length(x0)))\n_, yDeriv = eval_gen_gradient(f, x, xDot)\n    \n# output \n    \nyDeriv = [0.0, 0.0, -1.0]] \n\n\n\n\n\n","category":"function"},{"location":"functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference","text":"eval_compass_difference (f::Function, x::Vector{Float64}, ztol::Float64)\n\nCompute the function vector-valued y = f(x) and the compass difference of a scalar-valued f at x. If f has a domain dimension of 1 or 2, the compass difference is an element of Clarke's generalized gradient. \n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot. \nx::Vector{Float64}: input domain vector \n\nKeywords\n\nztol::Float64: capture radius for nonsmooth operations with kinks. Set to 0.0 by default.\n\nReturns\n\ny::Vector{Float64}: function value as a vector\nyCompass::Vector{Float64}: function compass difference \n\nExample\n\nTo calculate the compass difference of function f at x. \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\n_, yCompass = eval_compass_difference(f, x)\n    \n# output \n    \nyCompass = [0.5, 0.0, -0.5]\n\n\n\n\n\n","category":"function"},{"location":"functions.html#Functions-from-ConvexOptimization.jl","page":"Exported Functions","title":"Functions from ConvexOptimization.jl","text":"","category":"section"},{"location":"functions.html","page":"Exported Functions","title":"Exported Functions","text":"Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton\r\nMain.NonsmoothFwdAD.ConvexOptimization.LPNewton\r\nMain.NonsmoothFwdAD.ConvexOptimization.levelMethod","category":"page"},{"location":"functions.html#Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton","page":"Exported Functions","title":"Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton","text":"semiSmoothNewton(f::Function, x0::Vector{Float64}, kwargs…)\n\nCompute vector x where f(x) = 0 using the semi-smooth Newton method where there are multiple inputs to f but only one output. \n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot.\nx0::Vector{Float64}: initial domain vector guess \n\nKeywords\n\nepsilon::Float64: tolerance for solver stopping condition where epsilon + delta*norm(f(x0)). Set to 0.05 by default. \ndelta::Float64: tolerance for solver stopping condition where epsilon + delta*norm(f(x0)). Set to 0.00005 by default. \nmaxIter::Int64: maximum number of solver iterations. Set to 1000 by default. \n\n\n\n\n\n","category":"function"},{"location":"functions.html#Main.NonsmoothFwdAD.ConvexOptimization.LPNewton","page":"Exported Functions","title":"Main.NonsmoothFwdAD.ConvexOptimization.LPNewton","text":"LPNewton(f::Function, z0::Vector{Float64}, kwargs…)\n\nCompute vector z where f(z) = 0 using the Linear Program Newton method where the number of inputs to f is equivalent to number of outputs. \n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot.\nx0::Vector{Float64}: initial domain vector guess \n\nKeywords\n\nConstant and tolerance keyword\n\nepsilon::Float64: tolerance for solver stopping condition where f(z) >= epsilon. Set to 1e-2 by default. \nTOLERANCE::Float64: tolerance for JuMP solver. Set to 1e-6 by default.\nmaxIter::Int64: maximum number of solver iterations. Set to 20 by default. \n\nConstraint keywords\n\nlb::Vector{Float64}: lower bound on domain vector x. Set to -Inf vector of size n by default.\nub::Vector{Float64}: upper bound on domain vector x. Set to -Inf vector of size n by default.\nA::Matrix{Float64}: matrix A in inequality constraint set A*z <= b. Set to zeros vector of size n by default.\nb::Vector{Float64}: matrix b in inequality constraint set A*z <= b. Set to zeros matrix of size 1x1.\nAeq::Matrix{Float64}: matrix A in equality constraint set A*z == b. Set to zeros vector of size n by default. \nbeq::Vector{Float64}: matrix b in equality constraint set A*z == b. Set to zeros matrix of size 1x1.\n\nwhere n is the length of vector z0.\n\n\n\n\n\n","category":"function"},{"location":"functions.html#Main.NonsmoothFwdAD.ConvexOptimization.levelMethod","page":"Exported Functions","title":"Main.NonsmoothFwdAD.ConvexOptimization.levelMethod","text":"levelMethod(f::Function, z0::Vector{Float64}, kwargs…)\n\nCompute vector x to minimize f(x) using the Level method where there are multiple inputs to f but only one output. . \n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot.\nx0::Vector{Float64}: initial domain vector guess \n\nKeywords\n\nConstant and tolerance keywords\n\nepsilon::Float64: tolerance for solver stopping condition where f(z) >= epsilon. Set to 1e-2 by default. \nTOLERANCE::Float64: tolerance for JuMP solver. Set to 1e-6 by default.\nalpha::Float64: level set constant for quadratic programming. Set to 1/(2+sqrt(2) by default. \nmaxIter::Int64: maximum number of solver iterations. Set to 20 by default. \n\nConstraint keywords\n\nlb::Vector{Float64}: lower bound on domain vector x. Set to -Inf vector of size n by default.\nub::Vector{Float64}: upper bound on domain vector x. Set to -Inf vector of size n by default.\nA::Matrix{Float64}: matrix A in inequality constraint set A*z <= b. Set to zeros vector of size n by default.\nb::Vector{Float64}: matrix b in inequality constraint set A*z <= b. Set to zeros matrix of size 1x1.\nAeq::Matrix{Float64}: matrix A in equality constraint set A*z == b. Set to zeros vector of size n by default. \nbeq::Vector{Float64}: matrix b in equality constraint set A*z == b. Set to zeros matrix of size 1x1.\n\nwhere n is the length of vector z0.\n\n\n\n\n\n","category":"function"},{"location":"convexOptimization.html#ConvexOptimization-Implementation-Overview","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"","category":"section"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"There are three methods implemented in ConvexOptimization. ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"The first is the semi-smooth Newton's method - a technique for solving systems of the form f(x) = 0. ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"For smooth functions of 1 variable, it involves an iterative calculation of where the tangent line of x_k at f(x_k) intersects the x-axis at x_k+1. ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"The equation for the derivative f(x_k) is:","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"f(x_k) = fracf(x_k) - 0x_k - x_k+1","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"Solving for x_k+1 provides:","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"x_k+1 = x_k + fracf(x_k)f(x_k)","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"And x_k is re-calculated until f(x_k) = 0. ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"For semi-smooth functions:","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"Of 1 variable, f(x_k) is the generalized derivative as calculated by eval_gen_derivative(f,x).\nOf more than 1 variable, f(x_k) is the generalized gradient as calculated by eval_gen_gradient(f,x).","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"The second implemented method was the LP-Newton method proposed by Facchinei, Fischer, and Herrich (2013) for the convergence to a local solution for a constrained system of equations. It removes some of the original assumptions required by the original Newton's method (e.g., convexity). It is particular advantageous for KKT systems derived from optimality conditions for constrained optimization or variational inequalities.","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"For systems of the form F(z) = 0 where z ∈ Ω where Ω is a nonempty closed solution set - either whole space or polyhedral. It involves iteratively solving the following optimization problem for (z_k γ_k) to convergence:","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"min _z_k+1γ_k+1 (γ), ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"s.t. z  Ω","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"F(z_k) + G(z_k) cdot (z_k+1 - z_k)   γ_k+1F(z_k)^2,","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"z_k+1 - z_k   F(z_k) ,","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"$ γ_{k+1} ≥ 0$","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"Each iteration of the problem is solved using Ipopt.jl through Julia's JuMP. Note that the terms F(z_k) and G(z_k) are calculated using eval_gen_derivative(f, z) for each iteration. ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"The last method is the Level method for convex, nonsmooth minimizations. It minimizes f(x) for x by iteratively solving (1) an LP for the function subgradient and (2) a QP for x by minimizing the Euclidean norm. ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"An LP for the function subgradient: ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"min t,","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"s.t. f(x_i )+g(x_i )x-x_i ti=0k","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"x  Q ","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"where x is x^* and t is ^f^*(x)","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"A QP for x by minimizing the Euclidean norm:","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"min $ ||x-x_k ||^2$,","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"s.t. f(x_i )+g(x_i )x-x_i l_k (alpha)i=0k","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"x  Q","category":"page"},{"location":"convexOptimization.html","page":"ConvexOptimization Implementation Overview","title":"ConvexOptimization Implementation Overview","text":"where l_k(alpha) is (1-alpha)^f^*(x) + alpha^f^*(x)","category":"page"},{"location":"index.html#**NonSmoothFwdAD-Documentation**","page":"Introduction","title":"NonSmoothFwdAD Documentation","text":"","category":"section"},{"location":"index.html#Installation-and-Usage","page":"Introduction","title":"Installation and Usage","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"NonSmoothFwdAD is currently not a registered Julia package. To download from the Julia REPL, type ] to access Pkg REPL mode and then run the following command:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"add https://github.com/kamilkhanlab/nonsmooth-forward-ad ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Then, to use the package:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using NonSmoothFwdAD","category":"page"},{"location":"index.html#Example","page":"Introduction","title":"Example","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The usage of NonSmoothFwdAD is demonstrated by scripts test.jl and convexTest.jl. ","category":"page"},{"location":"index.html#GeneralizedDiff","page":"Introduction","title":"GeneralizedDiff","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Consider the following nonsmooth function of two variables, to replicate Example 6.2 from Khan and Barton (2013):","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"f(x) = max(min(x[1], -x[2]), x[2] - x[1])","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Using the NonsmoothFwdAD module (after include(\"NonsmoothFwdAD.jl\") and using .NonsmoothFwdAD, .GeneralizedDiff), we may evaluate a value y and a generalized gradient element yGrad of f at [0.0, 0.0] by the following alternative approaches, using the nonsmooth vector forward mode of AD.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"By defining f beforehand:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"    f(x) = max(min(x[1], -x[2]), x[2] - x[1])\r\n    y, yGrad = eval_gen_gradient(f, [0.0, 0.0])\r\n\r\n    #output \r\n    y = 0.0\r\n    yGrad = [0.0, -1.0]","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"By defining f as an anonymous function:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"    y, yGrad = eval_gen_gradient([0.0, 0.0]) do x\r\n        return max(min(x[1], -x[2]), x[2] - x[1])\r\n    end\t\r\n\r\n    #output \r\n    y = 0.0\r\n    yGrad = [0.0, -1.0]","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Here, eval_gen_gradient constructs yGrad as a Vector{Float64}, and only applies to scalar-valued functions. For vector-valued functions, eval_gen_derivative instead produces a generalized derivative element yDeriv::Matrix{Float64}.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"For scalar-valued functions of one or two variables, the \"compass difference\" is guaranteed to be an element of Clarke's generalized gradient. We may calculate the compass difference yCompass::Vector{Float64} for the above function f at [0.0, 0.0] as follows:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"_, yCompass = eval_compass_difference([0.0, 0.0]) do x\r\n    return max(min(x[1], -x[2]), x[2] - x[1])\r\nend\t","category":"page"},{"location":"index.html#ConvexOptimization","page":"Introduction","title":"ConvexOptimization","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Consider the following optimization problem, replicating Example 6 in F. Facchinei et. al (2014): ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"min PHI(x) = (x[1] + x[2])*x[4] + 0.5*(x[2] + x[3])^2 \r\n    s.t.    x[1] <= 0\r\n            x[2] >= 1\r\n            x[4] >= 0\r\n            x[1] + x[2] + x[3] >=0 ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The provided non-smooth reformulation to the Karush-Kuhn-Tucker system is as follows: ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"uOffset = 4\r\nvOffset = 9\r\nf(x) = [x[4] + x[1+uOffset] - x[2+uOffset] - x[5+uOffset],\r\n    x[4] + x[2] + x[3] - x[3+uOffset] - x[5+uOffset],\r\n    x[2] + x[3] - x[5+uOffset],\r\n    x[1] + x[2] - x[4+uOffset],\r\n    x[1] + x[1+vOffset],\r\n    - x[1] + x[2+vOffset],\r\n    1.0 - x[2] + x[3+vOffset],\r\n    - x[4] + x[4+vOffset],\r\n    - x[1] - x[2] - x[3] + x[5+vOffset],\r\n    min(x[1+uOffset], x[1+vOffset]),\r\n    min(x[2+uOffset], x[2+vOffset]),\r\n    min(x[3+uOffset], x[3+vOffset]),\r\n    min(x[4+uOffset], x[4+vOffset]),\r\n    min(x[5+uOffset], x[5+vOffset])] ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Using the NonsmoothFwdAD module (after include(\"NonsmoothFwdAD.jl\") and using .NonsmoothFwdAD, .GeneralizedDiff, .ConvexOptimization), the LPNewton method can locate the minima of (x, PHI(x)) by solving f(x) = 0 given no other binding set constraints. Assume an initial guess of x0 for f.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"x0 = [1.0, 4.0, -2.0, 1.0,\r\n    3.0, 3.0, 1.0, 4.0, 1.0,\r\n    0.0, 1.0, 3.0, 1.0, 3.0]\r\nx, _, gamma = LPNewton(f, x0)\r\n\r\n#output\r\n\r\nx = [0.0, 3.32, -3.32, 0.0, \r\n    2.76, 2.76, 0.0, 3.32, 0.0, \r\n    0.0, 0.0, 2.32, 0.0, 0.0]\r\ngamma = 4.99","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Thus, the solution for PHI(x) is (x = [0.0, 3.32, -3.32, 0.0], PHI(x) = 0.0). ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Note that the solution set of the optimization problem PHI(x) is X = {(0, t, −t, 0)|t ≥ 1}. Different initial guesses x0 could produce different local optima for PHI(x) where f(x) = 0. This is just one potential solution. ","category":"page"},{"location":"index.html#Authors","page":"Introduction","title":"Authors","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Maha Chaudhry, Department of Chemical Engineering, McMaster University\nKamil Khan, Department of Chemical Engineering, McMaster University","category":"page"},{"location":"index.html#References","page":"Introduction","title":"References","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"KA Khan and PI Barton, A vector forward mode of automatic differentiation for generalized derivative evaluation, Optimization Methods and Software, 30(6):1185-1212, 2015. DOI:10.1080/10556788.2015.1025400\nKA Khan and Y Yuan, Constructing a subgradient from directional derivatives for functions of two variables, Journal of Nonsmooth Analysis and Optimization, 1:6551, 2020. DOI:10.46298/jnsao-2020-6061\nPI Barton, KA Khan, P Stechlinski, and HAJ Watson, Computationally relevant generalized derivatives: theory, evaluation, and applications, Optimization Methods and Software, 33:1030-1072, 2017. DOI:10.1080/10556788.2017.1374385\nA Griewank, Automatic directional differentiation of nonsmooth composite functions, French-German Conference on Optimization, Dijon, 1994.\nA Griewank and A Walther, Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation (2nd ed.), SIAM, 2008.\nY Nesterov, Lexicographic differentiation of nonsmooth functions, Mathematical Programming, 104:669-700, 2005.\nF Facchinei, A Fischer, M Herrich, An LP-Newton method: nonsmooth equations, KKT systems, and nonisolated solutions, Mathematical Programming, 146:1-36, 2014, DOI: 10.1007/s10107-013-0676-6","category":"page"},{"location":"methodOverview.html#Method-Overview","page":"Method Overview","title":"Method Overview","text":"","category":"section"},{"location":"methodOverview.html","page":"Method Overview","title":"Method Overview","text":"Suppose we have a composite, non-smooth function f of n variables with derivative Df. ","category":"page"},{"location":"methodOverview.html","page":"Method Overview","title":"Method Overview","text":"The following implementation by Khan and Barton (2015) provides vector-forward AD evaluation methods for a generalized derivative Df(x)M of f on input domain vector x using a chosen matrix M through operator overloading.","category":"page"},{"location":"methodOverview.html","page":"Method Overview","title":"Method Overview","text":"The overloading defines additional calculus for elemental non-smooth functions, like abs, min, or max, based on “LD-derivative” constructions. This requires knowledge of when said elemental functions are at “kinks” – non-differentiable points. ","category":"page"},{"location":"methodOverview.html","page":"Method Overview","title":"Method Overview","text":"The resulting output derivative-matrix product is a valid generalized derivative with the same properties of an element of a Clarke’s generalized Jacobian. ","category":"page"},{"location":"methodOverview.html","page":"Method Overview","title":"Method Overview","text":"An additional “compass difference” evaluation procedure was added for bivariate scalar-valued functions. Khan and Yuan (2020) show generalized derivatives can be constructed using four directional derivative evaluations in a “compass difference” without the usual LD-derivative calculus. ","category":"page"}]
}
