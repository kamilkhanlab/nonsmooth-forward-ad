<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Exported Functions · NonSmoothFwdAD</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">NonSmoothFwdAD</a></span></div><form class="docs-search" action="search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="index.html">Introduction</a></li><li><a class="tocitem" href="methodOverview.html">Method Overview</a></li><li><a class="tocitem" href="convexOptimization.html">ConvexOptimization Implementation Overview</a></li><li class="is-active"><a class="tocitem" href="functions.html">Exported Functions</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Exported-Functions"><span>Exported Functions</span></a></li><li><a class="tocitem" href="#Functions-from-GeneralizedDiff.jl"><span>Functions from GeneralizedDiff.jl</span></a></li><li><a class="tocitem" href="#Functions-from-ConvexOptimization.jl"><span>Functions from ConvexOptimization.jl</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="functions.html">Exported Functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="functions.html">Exported Functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/main/docs/src/functions.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Exported-Structs"><a class="docs-heading-anchor" href="#Exported-Structs">Exported Structs</a><a id="Exported-Structs-1"></a><a class="docs-heading-anchor-permalink" href="#Exported-Structs" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.GeneralizedDiff.AFloat" href="#Main.NonsmoothFwdAD.GeneralizedDiff.AFloat"><code>Main.NonsmoothFwdAD.GeneralizedDiff.AFloat</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AFloat{val::Float64, dot::Vector{Float64}, ztol::Float64}</code></pre><p>Type representing generalized derivative information.</p><p><strong>Fields</strong></p><ul><li><code>val::Float64</code>: output value</li><li><code>dot::Vector64</code>: output generalized derivative. Set to <code>I</code>.</li><li><code>ztol::Float64</code>: capture radius for nonsmooth operations with kinks. Set to <code>0.0</code> by default.</li></ul><p><strong>Notes:</strong></p><p>Operator overloading macros defining generalized differentiation rules for elemental operations require AFloat inputs. Structs cannot be defined outside main module file. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/GeneralizedDiff.jl#L45-L58">source</a></section></article><h1 id="Exported-Functions"><a class="docs-heading-anchor" href="#Exported-Functions">Exported Functions</a><a id="Exported-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Exported-Functions" title="Permalink"></a></h1><h2 id="Functions-from-GeneralizedDiff.jl"><a class="docs-heading-anchor" href="#Functions-from-GeneralizedDiff.jl">Functions from GeneralizedDiff.jl</a><a id="Functions-from-GeneralizedDiff.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Functions-from-GeneralizedDiff.jl" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative" href="#Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative"><code>Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">eval_ld_derivative(f::Function, x::Vector{Float64}, xDot::T; ztol::Float64)</code></pre><p>Compute the function vector-valued f(x) and the LD-derivative f&#39;(x; xDot).</p><p><strong>Arguments</strong></p><ul><li><code>f::Function</code>: must be continous and of finite compositions of elemental operations. Each operation must be of the form <code>f(x::N)::Float64</code> where N is either <code>Vector{Float64}</code> or <code>Float64</code> ; otherwise implementation cannot map <code>f</code>. Supported elemental operations include: <code>+, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot</code>. </li><li><code>x::Vector{Float64}</code>: input domain vector </li><li><code>xDot::T</code>: output generalized derivative. It must have full row rank. Set to <code>I</code> by default.</li></ul><p>where <code>T</code> is either <code>Matrix{Float64}</code> or <code>Vector{Vector{Float64}}</code></p><p><strong>Keywords</strong></p><ul><li><code>ztol::Float64</code>: capture radius for nonsmooth operations with kinks. Set to <code>0.0</code> by default.</li></ul><p><strong>Returns</strong></p><ul><li><code>y::Vector{Float64}</code>: </li><li><code>yDot::T</code> where <code>T</code> is either <code>Matrix{Float64}</code> or <code>Vector{Vector{Float64}}</code>: the lexicographic derivative <code>f&#39;(x; xDot)</code> </li></ul><p><strong>Example</strong></p><p>To evaluate the LD-derivative of function <code>f</code> on vector <code>x</code>: </p><pre><code class="language-Julia hljs">f(x) = min(x[2]*x[3] + x[1], -x[3])
x = [0.0, 0.0, 0.0]
xDot = Matrix{Float64}(I(length(x0)))
_, yDot = eval_ld_derivative(f, x, xDot)

# output 

yDot = [0.0, 0.0, -1.0] </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/GeneralizedDiff.jl#L92-L128">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative" href="#Main.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative"><code>Main.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">eval_dir_derivative (f::Function, x::Vector{Float64}, xDot::Vector{Float64}; ztol::Float64)</code></pre><p>Compute the function vector-valued <code>y = f(x)</code> and the directional derivative <code>yDeriv = f&#39;(x; xDot)</code>. </p><p>See <code>eval_ld_derivative</code> for more details on function inputs.</p><p><strong>Returns</strong></p><ul><li><code>y::Vector{Float64}</code>: function value as a vector</li><li><code>yDot::Vector{Float64}</code>: directional derivative f&#39;(x; xDot::Vector) as a vector </li></ul><p><strong>Notes</strong></p><p>For this function, argument <code>xDot</code> must be of type <code>Vector{Float64}</code>. </p><p><strong>Example</strong></p><p>To calculate the directional derivative of function <code>f</code> in the direction of vector <code>x</code>. </p><pre><code class="language-Julia hljs">f(x) = min(x[2]*x[3] + x[1], -x[3])
x = [0.0, 0.0, 0.0]
xDot = [1.0, 0.0, 1.0]
_, yDeriv = eval_dir_derivative(f, x, xDot)

# output 

yDeriv = [-1.0] </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/GeneralizedDiff.jl#L201-L231">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative" href="#Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative"><code>Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">eval_gen_derivative (f::Function, x::Vector{Float64}, xDot::Matrix{Float64}; ztol::Float64)</code></pre><p>Compute the function vector-valued <code>y = f(x)</code> and the generalized derivative <code>yDeriv = Df(x; xDot)</code>. </p><p>See <code>eval_ld_derivative</code> for more details on function inputs.</p><p><strong>Returns</strong></p><ul><li><code>y::Vector{Float64}</code>: function value as a vector</li><li><code>yDeriv::Matrix{Float64}</code>: lexicographic derivative Df(x; xDot::Vector) as a vector </li></ul><p><strong>Notes</strong></p><p>For this function, argument <code>xDot</code> must be of type <code>Matrix{Float64}</code>. </p><p><strong>Example</strong></p><p>To calculate the generalized derivative of function <code>f</code> in the direction of vector <code>x</code>. </p><pre><code class="language-Julia hljs">f(x) = min(x[2]*x[3] + x[1], -x[3])
x = [0.0, 0.0, 0.0]
xDot = Matrix{Float64}(I(length(x0)))
_, yDeriv = eval_gen_derivative(f, x, xDot)
    
# output 
    
yDeriv = [0.0, 0.0, -1.0]] </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/GeneralizedDiff.jl#L248-L279">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient" href="#Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient"><code>Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">eval_gen_gradient (f::Function, x::Vector{Float64}, xDot::Matrix{Float64}; ztol::Float64)</code></pre><p>Compute the function vector-valued <code>y = f(x)</code> and the generalized gradient <code>yGrad = yDeriv&#39;</code> for scalar valued functions <code>f</code>.</p><p>See <code>eval_ld_derivative</code> for more details on function inputs.</p><p><strong>Returns</strong></p><ul><li><code>y::Vector{Float64}</code>: function value as a vector</li><li><code>yGrad:Vector{Float64}</code>: directional derivative f&#39;(x; xDot::Vector) as a vector </li></ul><p><strong>Notes</strong></p><p>For this function, argument <code>xDot</code> must be of type <code>Matrix{Float64}</code>. </p><p><strong>Example</strong></p><p>To calculate the generalized gradient of function <code>f</code> in the direction of vector <code>x</code>. </p><pre><code class="language-Julia hljs">f(x) = min(x[2]*x[3] + x[1], -x[3])
x = [0.0, 0.0, 0.0]
xDot = Matrix{Float64}(I(length(x0)))
_, yDeriv = eval_gen_gradient(f, x, xDot)
    
# output 
    
yDeriv = [0.0, 0.0, -1.0]] </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/GeneralizedDiff.jl#L306-L337">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference" href="#Main.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference"><code>Main.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">eval_compass_difference (f::Function, x::Vector{Float64}, ztol::Float64)</code></pre><p>Compute the function vector-valued <code>y = f(x)</code> and the compass difference of a scalar-valued <code>f</code> at <code>x</code>. If <code>f</code> has a domain dimension of 1 or 2, the compass difference is an element of Clarke&#39;s generalized gradient. </p><p><strong>Arguments</strong></p><ul><li><code>f::Function</code>: must be continous and of finite compositions of elemental operations. Each operation must be of the form <code>f(x::N)::Float64</code> where N is either <code>Vector{Float64}</code> or <code>Float64</code> ; otherwise implementation cannot map <code>f</code>. Supported elemental operations include: <code>+, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot</code>. </li><li><code>x::Vector{Float64}</code>: input domain vector </li></ul><p><strong>Keywords</strong></p><ul><li><code>ztol::Float64</code>: capture radius for nonsmooth operations with kinks. Set to <code>0.0</code> by default.</li></ul><p><strong>Returns</strong></p><ul><li><code>y::Vector{Float64}</code>: function value as a vector</li><li><code>yCompass::Vector{Float64}</code>: function compass difference </li></ul><p><strong>Example</strong></p><p>To calculate the compass difference of function <code>f</code> at <code>x</code>. </p><pre><code class="language-Julia hljs">f(x) = min(x[2]*x[3] + x[1], -x[3])
x = [0.0, 0.0, 0.0]
_, yCompass = eval_compass_difference(f, x)
    
# output 
    
yCompass = [0.5, 0.0, -0.5]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/GeneralizedDiff.jl#L367-L400">source</a></section></article><h2 id="Functions-from-ConvexOptimization.jl"><a class="docs-heading-anchor" href="#Functions-from-ConvexOptimization.jl">Functions from ConvexOptimization.jl</a><a id="Functions-from-ConvexOptimization.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Functions-from-ConvexOptimization.jl" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton" href="#Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton"><code>Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">semiSmoothNewton(f::Function, x0::Vector{Float64}, kwargs…)</code></pre><p>Compute vector <code>x</code> where <code>f(x) = 0</code> using the semi-smooth Newton method where there are multiple inputs to <code>f</code> but only one output. </p><p><strong>Arguments</strong></p><ul><li><code>f::Function</code>: must be continous and of finite compositions of elemental operations. Each operation must be of the form <code>f(x::N)::Float64</code> where N is either <code>Vector{Float64}</code> or <code>Float64</code> ; otherwise implementation cannot map <code>f</code>. Supported elemental operations include: <code>+, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot</code>.</li><li><code>x0::Vector{Float64}</code>: initial domain vector guess </li></ul><p><strong>Keywords</strong></p><ul><li><code>epsilon::Float64</code>: tolerance for solver stopping condition where <code>epsilon + delta*norm(f(x0))</code>. Set to <code>0.05</code> by default. </li><li><code>delta::Float64</code>: tolerance for solver stopping condition where <code>epsilon + delta*norm(f(x0))</code>. Set to <code>0.00005</code> by default. </li><li><code>maxIter::Int64</code>: maximum number of solver iterations. Set to <code>1000</code> by default. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/ConvexOptimization.jl#L12-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.ConvexOptimization.LPNewton" href="#Main.NonsmoothFwdAD.ConvexOptimization.LPNewton"><code>Main.NonsmoothFwdAD.ConvexOptimization.LPNewton</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">LPNewton(f::Function, z0::Vector{Float64}, kwargs…)</code></pre><p>Compute vector <code>z</code> where <code>f(z) = 0</code> using the Linear Program Newton method where the number of inputs to <code>f</code> is equivalent to number of outputs. </p><p><strong>Arguments</strong></p><ul><li><code>f::Function</code>: must be continous and of finite compositions of elemental operations. Each operation must be of the form <code>f(x::N)::Float64</code> where N is either <code>Vector{Float64}</code> or <code>Float64</code> ; otherwise implementation cannot map <code>f</code>. Supported elemental operations include: <code>+, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot</code>.</li><li><code>x0::Vector{Float64}</code>: initial domain vector guess </li></ul><p><strong>Keywords</strong></p><p><strong>Constant and tolerance keyword</strong></p><ul><li><code>epsilon::Float64</code>: tolerance for solver stopping condition where <code>f(z) &gt;= epsilon</code>. Set to <code>1e-2</code> by default. </li><li><code>TOLERANCE::Float64</code>: tolerance for JuMP solver. Set to <code>1e-6</code> by default.</li><li><code>maxIter::Int64</code>: maximum number of solver iterations. Set to <code>20</code> by default. </li></ul><p><strong>Constraint keywords</strong></p><ul><li><code>lb::Vector{Float64}</code>: lower bound on domain vector <code>x</code>. Set to <code>-Inf</code> vector of size <code>n</code> by default.</li><li><code>ub::Vector{Float64}</code>: upper bound on domain vector <code>x</code>. Set to <code>-Inf</code> vector of size <code>n</code> by default.</li><li><code>A::Matrix{Float64}</code>: matrix <code>A</code> in inequality constraint set <code>A*z &lt;= b</code>. Set to zeros vector of size <code>n</code> by default.</li><li><code>b::Vector{Float64}</code>: matrix <code>b</code> in inequality constraint set <code>A*z &lt;= b</code>. Set to zeros matrix of size <code>1x1</code>.</li><li><code>Aeq::Matrix{Float64}</code>: matrix <code>A</code> in equality constraint set <code>A*z == b</code>. Set to zeros vector of size <code>n</code> by default. </li><li><code>beq::Vector{Float64}</code>: matrix <code>b</code> in equality constraint set <code>A*z == b</code>. Set to zeros matrix of size <code>1x1</code>.</li></ul><p>where <code>n</code> is the length of vector <code>z0</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/ConvexOptimization.jl#L66-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Main.NonsmoothFwdAD.ConvexOptimization.levelMethod" href="#Main.NonsmoothFwdAD.ConvexOptimization.levelMethod"><code>Main.NonsmoothFwdAD.ConvexOptimization.levelMethod</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">levelMethod(f::Function, z0::Vector{Float64}, kwargs…)</code></pre><p>Compute vector <code>x</code> to minimize <code>f(x)</code> using the Level method where there are multiple inputs to <code>f</code> but only one output. . </p><p><strong>Arguments</strong></p><ul><li><code>f::Function</code>: must be continous and of finite compositions of elemental operations. Each operation must be of the form <code>f(x::N)::Float64</code> where N is either <code>Vector{Float64}</code> or <code>Float64</code> ; otherwise implementation cannot map <code>f</code>. Supported elemental operations include: <code>+, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot</code>.</li><li><code>x0::Vector{Float64}</code>: initial domain vector guess </li></ul><p><strong>Keywords</strong></p><p><strong>Constant and tolerance keywords</strong></p><ul><li><code>epsilon::Float64</code>: tolerance for solver stopping condition where <code>f(z) &gt;= epsilon</code>. Set to <code>1e-2</code> by default. </li><li><code>TOLERANCE::Float64</code>: tolerance for JuMP solver. Set to <code>1e-6</code> by default.</li><li><code>alpha::Float64</code>: level set constant for quadratic programming. Set to <code>1/(2+sqrt(2)</code> by default. </li><li><code>maxIter::Int64</code>: maximum number of solver iterations. Set to <code>20</code> by default. </li></ul><p><strong>Constraint keywords</strong></p><ul><li><code>lb::Vector{Float64}</code>: lower bound on domain vector <code>x</code>. Set to <code>-Inf</code> vector of size <code>n</code> by default.</li><li><code>ub::Vector{Float64}</code>: upper bound on domain vector <code>x</code>. Set to <code>-Inf</code> vector of size <code>n</code> by default.</li><li><code>A::Matrix{Float64}</code>: matrix <code>A</code> in inequality constraint set <code>A*z &lt;= b</code>. Set to zeros vector of size <code>n</code> by default.</li><li><code>b::Vector{Float64}</code>: matrix <code>b</code> in inequality constraint set <code>A*z &lt;= b</code>. Set to zeros matrix of size <code>1x1</code>.</li><li><code>Aeq::Matrix{Float64}</code>: matrix <code>A</code> in equality constraint set <code>A*z == b</code>. Set to zeros vector of size <code>n</code> by default. </li><li><code>beq::Vector{Float64}</code>: matrix <code>b</code> in equality constraint set <code>A*z == b</code>. Set to zeros matrix of size <code>1x1</code>.</li></ul><p>where <code>n</code> is the length of vector <code>z0</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kamilkhanlab/nonsmooth-forward-ad/blob/ecdf639e5ce34a18bd2571eddf84a40c65725c6f/src/ConvexOptimization.jl#L164-L194">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="convexOptimization.html">« ConvexOptimization Implementation Overview</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Friday 24 March 2023 14:50">Friday 24 March 2023</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
