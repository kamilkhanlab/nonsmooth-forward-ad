var documenterSearchIndex = {"docs":
[{"location":"generalizedDiff/methodOverview.html#Method-Overview","page":"Method Overview","title":"Method Overview","text":"","category":"section"},{"location":"generalizedDiff/methodOverview.html","page":"Method Overview","title":"Method Overview","text":"The standard vector forward mode of automatic differentiation (AD) evaluates derivative-matrix products efficiently for composite smooth functions, and is described by Griewank and Walther (2008). For a composite smooth function f of n variables, and with derivative Df, the vector forward AD mode takes a domain vector x and a matrix M as input, and produces the product Df(x) M as output. To do this, the method regards f as a composition of simple elemental functions (such as the arithmetic operations +/-/*// and trigonometric functions), and handles each elemental function using the standard chain rule for differentiation.","category":"page"},{"location":"generalizedDiff/methodOverview.html","page":"Method Overview","title":"Method Overview","text":"For nonsmooth functions, this becomes more complicated. While generalized derivatives such as Clarke's generalized Jacobian are well-defined for continuous functions that are not differentiable everywhere, they have traditionally been considered difficult to evaluate for composite nonsmooth functions, due to failure of classical chain rules. We expect a \"correct\" generalized derivative to be the actual derivative/gradient when an ostensibly nonsmooth function is in fact smooth, and to be an actual subgradient when the function is convex. Naive extensions of AD to nonsmooth functions, however, do not have these properties.","category":"page"},{"location":"generalizedDiff/methodOverview.html","page":"Method Overview","title":"Method Overview","text":"Khan and Barton (2012, 2013, 2015) showed that the vector forward AD mode can be generalized to handle composite nonsmooth functions, by defining additional calculus rules for elemental nonsmooth functions such as abs, min, max, and the Euclidean norm. These calculus rules are based on a new construction called the \"LD-derivative\", which is a variant of an earlier construction by Nesterov (2005). The resulting \"derivative\" in the output derivative-matrix product is a valid generalized derivative for use in methods for nonsmooth optimization and equation-solving, with essentially the same properties as an element of Clarke's generalized Jacobian. In particular:","category":"page"},{"location":"generalizedDiff/methodOverview.html","page":"Method Overview","title":"Method Overview","text":"if the function is smooth, then this method will compute the actual derivative,\nif the function is convex, then a subgradient will be computed, \nif no multivariate Euclidean norms are present, then an element of Clarke's generalized Jacobian will be computed,\nin all cases, an element of Nesterov's lexicographic derivative will be computed.","category":"page"},{"location":"generalizedDiff/methodOverview.html","page":"Method Overview","title":"Method Overview","text":"Khan and Barton's nonsmooth vector forward AD mode is also a generalization of a directional derivative evaluation method by Griewank (1994); Griewank's method is recovered when the chosen matrix M has only one column. See Barton et al. (2017) for further discussion of applications and extensions of the nonsmooth vector forward AD mode. ","category":"page"},{"location":"generalizedDiff/methodOverview.html","page":"Method Overview","title":"Method Overview","text":"Khan and Yuan (2020) showed that, for bivariate scalar-valued functions that are locally Lipschitz continuous and directionally differentiable, valid generalized derivatives may be constructed by assembling four directional derivative evaluations into a so-called \"compass difference\", without the LD-derivative calculus required in the nonsmooth vector forward AD mode. ","category":"page"},{"location":"generalizedDiff/methodOverview.html#References","page":"Method Overview","title":"References","text":"","category":"section"},{"location":"generalizedDiff/methodOverview.html","page":"Method Overview","title":"Method Overview","text":"KA Khan and PI Barton, A vector forward mode of automatic differentiation for generalized derivative evaluation, Optimization Methods and Software, 30(6):1185-1212, 2015. DOI:10.1080/10556788.2015.1025400\nKA Khan and Y Yuan, Constructing a subgradient from directional derivatives for functions of two variables, Journal of Nonsmooth Analysis and Optimization, 1:6551, 2020. DOI:10.46298/jnsao-2020-6061\nPI Barton, KA Khan, P Stechlinski, and HAJ Watson, Computationally relevant generalized derivatives: theory, evaluation, and applications, Optimization Methods and Software, 33:1030-1072, 2017. DOI:10.1080/10556788.2017.1374385\nA Griewank, Automatic directional differentiation of nonsmooth composite functions, French-German Conference on Optimization, Dijon, 1994.\nA Griewank and A Walther, Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation (2nd ed.), SIAM, 2008.\nY Nesterov, Lexicographic differentiation of nonsmooth functions, Mathematical Programming, 104:669-700, 2005.","category":"page"},{"location":"convexOptimization/implementationOverview.html#Implementation-Overview","page":"Implementation Overview","title":"Implementation Overview","text":"","category":"section"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"There are three methods implemented in ConvexOptimization. ","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"The first is the semi-smooth Newton's method - a technique for solving systems of the form f(x) = 0. ","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"For smooth functions of 1 variable, it involves an iterative calculation of where the tangent line of x_k at f(x_k) intersects the x-axis at x_k+1. ","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"The equation for the derivative f(x_k) is:","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"f(x_k) = fracf(x_k) - 0x_k - x_k+1","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"Solving for x_k+1 provides:","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"x_k+1 = x_k + fracf(x_k)f(x_k)","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"And x_k is re-calculated until f(x_k) = 0. ","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"For semi-smooth functions:","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"Of 1 variable, f(x_k) is the generalized derivative as calculated by eval_gen_derivative(f,x).\nOf more than 1 variable, f(x_k) is the generalized gradient as calculated by eval_gen_gradient(f,x).","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"The second implemented method was the LP-Newton method proposed by Facchinei, Fischer, and Herrich (2013) for the convergence to a local solution for a constrained system of equations. It removes some of the original assumptions required by the original Newton's method (e.g., convexity). It is particular advantageous for KKT systems derived from optimality conditions for constrained optimization or variational inequalities.","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"For systems of the form F(z) = 0 where z ∈ Ω where Ω is a nonempty closed solution set - either whole space or polyhedral. It involves iteratively solving the following optimization problem for (z_k γ_k) to convergence:","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"min _z_k+1 (γ_k+1), ","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"subject to: ","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"z  Ω\nF(z_k) + G(z_k) cdot (z_k+1 - z_k)   γ_k+1F(z_k)^2,\nz_k+1 - z_k   F(z_k) ,\n$ γ_{k+1} ≥ 0$","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"Each iteration of the problem is solved using Ipopt.jl through Julia's JuMP. Note that the terms F(z_k) and G(z_k) are calculated using eval_gen_derivative(f, z) for each iteration. ","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"The last method is the Level method for convex, nonsmooth minimizations. It minimizes f(x) for x by iteratively solving (1) an LP for the function subgradient and (2) a QP for x by minimizing the Euclidean norm. ","category":"page"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"An LP for the function subgradient: \nmin _x^* (t), \nsubject to:\nf(x_i )+g(x_i )x^*-x_i t i=0k\nx  Q \nwhere t is ^f^*(x)\nA QP for x by minimizing the Euclidean norm:\nmin _x_k x_i-x_k ^2,\nsubject to:\nf(x_i )+g(x_i )x_k - x_i l_k (alpha)i=0k \nx  Q \nwhere l_k(alpha) is (1-alpha)^f^*(x^*) + alpha^f^*(x^*)","category":"page"},{"location":"convexOptimization/implementationOverview.html#References","page":"Implementation Overview","title":"References","text":"","category":"section"},{"location":"convexOptimization/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"Facchinei, A Fischer, M Herrich, An LP-Newton method: nonsmooth equations, KKT systems, and nonisolated solutions, Mathematical Programming, 146:1-36, 2014, DOI: 10.1007/s10107-013-0676-6\nNesterov, Y., Lectures on Convex Optimization (2nd ed.), SOIA, 2010, DOI: 10.1007/978-3-319-91578-4","category":"page"},{"location":"generalizedDiff/implementationOverview.html#Implementation-Overview","page":"Implementation Overview","title":"Implementation Overview","text":"","category":"section"},{"location":"generalizedDiff/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"The provided GeneralizedDiff module in GeneralizedDiff.jl performs both the nonsmooth vector forward AD mode and compass difference evaluation mentioned above, using operator overloading to carry out nonsmooth calculus rules automatically and efficiently. The implementation is structured just like the archetypal vector forward AD mode described by Griewank and Walther (2008), but with additional handling of nonsmooth functions.","category":"page"},{"location":"generalizedDiff/implementationOverview.html#Usage","page":"Implementation Overview","title":"Usage","text":"","category":"section"},{"location":"generalizedDiff/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"The script test.jl illustrates the usage of GeneralizedDiff, and evaluates generalized derivatives for several nonsmooth functions. This module defines calculus rules for the following scalar-valued elemental operations via operator operloading:","category":"page"},{"location":"generalizedDiff/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"+, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot","category":"page"},{"location":"generalizedDiff/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"The differentiation methods of this module apply to compositions of these operations. Additional univariate operations may be included by adapting the handling of log or abs, and additional bivariate operations may be included by adapting the handling of * or hypot. The overloaded ^ operation only supports integer-valued exponents; rewrite non-integer exponents as e.g. x^y = exp(y*log(x)).","category":"page"},{"location":"generalizedDiff/implementationOverview.html#Handling-nonsmoothness","page":"Implementation Overview","title":"Handling nonsmoothness","text":"","category":"section"},{"location":"generalizedDiff/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"The nonsmooth calculus rules used here are described by Khan and Barton (2015) and Barton et al. (2017). In particular, they require knowledge of when a nonsmooth elemental function like abs is exactly at its \"kink\" or not, which is difficult using floating point arithmetic. This implementation, by default, considers any domain point within an absolute tolerance of 1e-08 of a kink to be at that kink. In all of the exported functions listed above, this tolerance may be edited via a keyword argument ztol. For example, when using eval_gen_gradient, we could write:","category":"page"},{"location":"generalizedDiff/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"y, yGrad = eval_gen_gradient(f, x, ztol=1e-5)","category":"page"},{"location":"generalizedDiff/implementationOverview.html#References","page":"Implementation Overview","title":"References","text":"","category":"section"},{"location":"generalizedDiff/implementationOverview.html","page":"Implementation Overview","title":"Implementation Overview","text":"KA Khan and PI Barton, A vector forward mode of automatic differentiation for generalized derivative evaluation, Optimization Methods and Software, 30(6):1185-1212, 2015. DOI:10.1080/10556788.2015.1025400\nPI Barton, KA Khan, P Stechlinski, and HAJ Watson, Computationally relevant generalized derivatives: theory, evaluation, and applications, Optimization Methods and Software, 33:1030-1072, 2017. DOI:10.1080/10556788.2017.1374385\nA Griewank and A Walther, Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation (2nd ed.), SIAM, 2008.","category":"page"},{"location":"index.html#**NonSmoothFwdAD-Documentation**","page":"Introduction","title":"NonSmoothFwdAD Documentation","text":"","category":"section"},{"location":"index.html#Installation-and-Usage","page":"Introduction","title":"Installation and Usage","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"NonSmoothFwdAD is currently not a registered Julia package. To download from the Julia REPL, type ] to access Pkg REPL mode and then run the following command:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"add https://github.com/kamilkhanlab/nonsmooth-forward-ad ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Then, to use the package:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using NonSmoothFwdAD","category":"page"},{"location":"index.html#Example","page":"Introduction","title":"Example","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The usage of NonSmoothFwdAD is demonstrated by scripts test.jl and convexTest.jl. ","category":"page"},{"location":"index.html#GeneralizedDiff","page":"Introduction","title":"GeneralizedDiff","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Consider the following nonsmooth function of two variables, to replicate Example 6.2 from Khan and Barton (2013):","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"f(x) = max(min(x[1], -x[2]), x[2] - x[1])","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Using the NonsmoothFwdAD module (after include(\"NonsmoothFwdAD.jl\") and using .NonsmoothFwdAD, .GeneralizedDiff), we may evaluate a value y and a generalized gradient element yGrad of f at [0.0, 0.0] by the following alternative approaches, using the nonsmooth vector forward mode of AD.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"By defining f beforehand:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"    f(x) = max(min(x[1], -x[2]), x[2] - x[1])\r\n    y, yGrad = eval_gen_gradient(f, [0.0, 0.0])\r\n\r\n    #output \r\n    y = 0.0\r\n    yGrad = [0.0, -1.0]","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"By defining f as an anonymous function:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"    y, yGrad = eval_gen_gradient([0.0, 0.0]) do x\r\n        return max(min(x[1], -x[2]), x[2] - x[1])\r\n    end\t\r\n\r\n    #output \r\n    y = 0.0\r\n    yGrad = [0.0, -1.0]","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Here, eval_gen_gradient constructs yGrad as a Vector{Float64}, and only applies to scalar-valued functions. For vector-valued functions, eval_gen_derivative instead produces a generalized derivative element yDeriv::Matrix{Float64}.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"For scalar-valued functions of one or two variables, the \"compass difference\" is guaranteed to be an element of Clarke's generalized gradient. We may calculate the compass difference yCompass::Vector{Float64} for the above function f at [0.0, 0.0] as follows:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"_, yCompass = eval_compass_difference([0.0, 0.0]) do x\r\n    return max(min(x[1], -x[2]), x[2] - x[1])\r\nend\t","category":"page"},{"location":"index.html#ConvexOptimization","page":"Introduction","title":"ConvexOptimization","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Consider the following optimization problem, replicating Example 6 in F. Facchinei et. al (2014): ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"min PHI(x) = (x[1] + x[2])*x[4] + 0.5*(x[2] + x[3])^2 \r\n    s.t.    x[1] <= 0\r\n            x[2] >= 1\r\n            x[4] >= 0\r\n            x[1] + x[2] + x[3] >=0 ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The provided non-smooth reformulation to the Karush-Kuhn-Tucker system is as follows: ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"uOffset = 4\r\nvOffset = 9\r\nf(x) = [x[4] + x[1+uOffset] - x[2+uOffset] - x[5+uOffset],\r\n    x[4] + x[2] + x[3] - x[3+uOffset] - x[5+uOffset],\r\n    x[2] + x[3] - x[5+uOffset],\r\n    x[1] + x[2] - x[4+uOffset],\r\n    x[1] + x[1+vOffset],\r\n    - x[1] + x[2+vOffset],\r\n    1.0 - x[2] + x[3+vOffset],\r\n    - x[4] + x[4+vOffset],\r\n    - x[1] - x[2] - x[3] + x[5+vOffset],\r\n    min(x[1+uOffset], x[1+vOffset]),\r\n    min(x[2+uOffset], x[2+vOffset]),\r\n    min(x[3+uOffset], x[3+vOffset]),\r\n    min(x[4+uOffset], x[4+vOffset]),\r\n    min(x[5+uOffset], x[5+vOffset])] ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Using the NonsmoothFwdAD module (after include(\"NonsmoothFwdAD.jl\") and using .NonsmoothFwdAD, .GeneralizedDiff, .ConvexOptimization), the LPNewton method can locate the minima of (x, PHI(x)) by solving f(x) = 0 given no other binding set constraints. Assume an initial guess of x0 for f.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"x0 = [1.0, 4.0, -2.0, 1.0,\r\n    3.0, 3.0, 1.0, 4.0, 1.0,\r\n    0.0, 1.0, 3.0, 1.0, 3.0]\r\nx, _, gamma = LPNewton(f, x0)\r\n\r\n#output\r\n\r\nx = [0.0, 3.32, -3.32, 0.0, \r\n    2.76, 2.76, 0.0, 3.32, 0.0, \r\n    0.0, 0.0, 2.32, 0.0, 0.0]\r\ngamma = 4.99","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Thus, the solution for PHI(x) is (x = [0.0, 3.32, -3.32, 0.0], PHI(x) = 0.0). ","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Note that the solution set of the optimization problem PHI(x) is X = {(0, t, −t, 0)|t ≥ 1}. Different initial guesses x0 could produce different local optima for PHI(x) where f(x) = 0. This is just one potential solution. ","category":"page"},{"location":"index.html#Authors","page":"Introduction","title":"Authors","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Maha Chaudhry, Department of Chemical Engineering, McMaster University\nKamil Khan, Department of Chemical Engineering, McMaster University","category":"page"},{"location":"index.html#References","page":"Introduction","title":"References","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"KA Khan and PI Barton, A vector forward mode of automatic differentiation for generalized derivative evaluation, Optimization Methods and Software, 30(6):1185-1212, 2015. DOI:10.1080/10556788.2015.1025400\nKA Khan and Y Yuan, Constructing a subgradient from directional derivatives for functions of two variables, Journal of Nonsmooth Analysis and Optimization, 1:6551, 2020. DOI:10.46298/jnsao-2020-6061\nPI Barton, KA Khan, P Stechlinski, and HAJ Watson, Computationally relevant generalized derivatives: theory, evaluation, and applications, Optimization Methods and Software, 33:1030-1072, 2017. DOI:10.1080/10556788.2017.1374385\nA Griewank, Automatic directional differentiation of nonsmooth composite functions, French-German Conference on Optimization, Dijon, 1994.\nA Griewank and A Walther, Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation (2nd ed.), SIAM, 2008.\nY Nesterov, Lexicographic differentiation of nonsmooth functions, Mathematical Programming, 104:669-700, 2005.\nF Facchinei, A Fischer, M Herrich, An LP-Newton method: nonsmooth equations, KKT systems, and nonisolated solutions, Mathematical Programming, 146:1-36, 2014, DOI: 10.1007/s10107-013-0676-6","category":"page"},{"location":"generalizedDiff/functions.html#Exported-Functions","page":"Exported Functions","title":"Exported Functions","text":"","category":"section"},{"location":"generalizedDiff/functions.html#Exported-Structs","page":"Exported Functions","title":"Exported Structs","text":"","category":"section"},{"location":"generalizedDiff/functions.html","page":"Exported Functions","title":"Exported Functions","text":"Main.NonsmoothFwdAD.GeneralizedDiff.AFloat","category":"page"},{"location":"generalizedDiff/functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.AFloat","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.AFloat","text":"AFloat{val::Float64, dot::Vector{Float64}, ztol::Float64}\n\nType representing generalized derivative information analogous to the adouble class described by Griewank and Walther (2008).\n\nFields\n\nval::Float64: output value\ndot::Vector64: output generalized derivative. Set to I.\nztol::Float64: capture radius for nonsmooth operations with kinks. Set to 0.0 by default.\n\nNotes:\n\nOperator overloading macros defining generalized differentiation rules for elemental operations require AFloat inputs. Structs cannot be defined outside main module file. \n\n\n\n\n\n","category":"type"},{"location":"generalizedDiff/functions.html#Exported-Functions-2","page":"Exported Functions","title":"Exported Functions","text":"","category":"section"},{"location":"generalizedDiff/functions.html","page":"Exported Functions","title":"Exported Functions","text":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative\r\nMain.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative\r\nMain.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative\r\nMain.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient\r\nMain.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference","category":"page"},{"location":"generalizedDiff/functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_ld_derivative","text":"eval_ld_derivative(f::Function, x::Vector{Float64}, xDot::T; ztol::Float64)\n\nCompute the function vector-valued f(x) and the LD-derivative f'(x; xDot).\n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot. \nx::Vector{Float64}: input domain vector \nxDot::T: output generalized derivative. It must have full row rank. Set to I by default.\n\nwhere T is either Matrix{Float64} or Vector{Vector{Float64}}\n\nKeywords\n\nztol::Float64: capture radius for nonsmooth operations with kinks. Set to 0.0 by default.\n\nReturns\n\ny::Vector{Float64}: \nyDot::T where T is either Matrix{Float64} or Vector{Vector{Float64}}: the lexicographic derivative f'(x; xDot) \n\nExample\n\nTo evaluate the LD-derivative of function f on vector x: \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\nxDot = Matrix{Float64}(I(length(x0)))\n_, yDot = eval_ld_derivative(f, x, xDot)\n\n# output \n\nyDot = [0.0, 0.0, -1.0] \n\n\n\n\n\n","category":"function"},{"location":"generalizedDiff/functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_dir_derivative","text":"eval_dir_derivative (f::Function, x::Vector{Float64}, xDot::Vector{Float64}; ztol::Float64)\n\nCompute the function vector-valued y = f(x) and the directional derivative yDeriv = f'(x; xDot). \n\nSee eval_ld_derivative for more details on function inputs.\n\nReturns\n\ny::Vector{Float64}: function value as a vector\nyDot::Vector{Float64}: directional derivative f'(x; xDot::Vector) as a vector \n\nNotes\n\nFor this function, argument xDot must be of type Vector{Float64}. \n\nExample\n\nTo calculate the directional derivative of function f in the direction of vector x. \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\nxDot = [1.0, 0.0, 1.0]\n_, yDeriv = eval_dir_derivative(f, x, xDot)\n\n# output \n\nyDeriv = [-1.0] \n\n\n\n\n\n","category":"function"},{"location":"generalizedDiff/functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_derivative","text":"eval_gen_derivative (f::Function, x::Vector{Float64}, xDot::Matrix{Float64}; ztol::Float64)\n\nCompute the function vector-valued y = f(x) and the generalized derivative yDeriv = Df(x; xDot). \n\nSee eval_ld_derivative for more details on function inputs.\n\nReturns\n\ny::Vector{Float64}: function value as a vector\nyDeriv::Matrix{Float64}: lexicographic derivative Df(x; xDot::Vector) as a vector \n\nNotes\n\nFor this function, argument xDot must be of type Matrix{Float64}. \n\nExample\n\nTo calculate the generalized derivative of function f in the direction of vector x. \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\nxDot = Matrix{Float64}(I(length(x0)))\n_, yDeriv = eval_gen_derivative(f, x, xDot)\n    \n# output \n    \nyDeriv = [0.0, 0.0, -1.0]] \n\n\n\n\n\n","category":"function"},{"location":"generalizedDiff/functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_gen_gradient","text":"eval_gen_gradient (f::Function, x::Vector{Float64}, xDot::Matrix{Float64}; ztol::Float64)\n\nCompute the function vector-valued y = f(x) and the generalized gradient yGrad = yDeriv' for scalar valued functions f.\n\nSee eval_ld_derivative for more details on function inputs.\n\nReturns\n\ny::Vector{Float64}: function value as a vector\nyGrad:Vector{Float64}: directional derivative f'(x; xDot::Vector) as a vector \n\nNotes\n\nFor this function, argument xDot must be of type Matrix{Float64}. \n\nExample\n\nTo calculate the generalized gradient of function f in the direction of vector x. \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\nxDot = Matrix{Float64}(I(length(x0)))\n_, yDeriv = eval_gen_gradient(f, x, xDot)\n    \n# output \n    \nyDeriv = [0.0, 0.0, -1.0]] \n\n\n\n\n\n","category":"function"},{"location":"generalizedDiff/functions.html#Main.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference","page":"Exported Functions","title":"Main.NonsmoothFwdAD.GeneralizedDiff.eval_compass_difference","text":"eval_compass_difference (f::Function, x::Vector{Float64}, ztol::Float64)\n\nCompute the function vector-valued y = f(x) and the compass difference of a scalar-valued f at x. If f has a domain dimension of 1 or 2, the compass difference is an element of Clarke's generalized gradient. \n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot. \nx::Vector{Float64}: input domain vector \n\nKeywords\n\nztol::Float64: capture radius for nonsmooth operations with kinks. Set to 0.0 by default.\n\nReturns\n\ny::Vector{Float64}: function value as a vector\nyCompass::Vector{Float64}: function compass difference \n\nExample\n\nTo calculate the compass difference of function f at x. \n\nf(x) = min(x[2]*x[3] + x[1], -x[3])\nx = [0.0, 0.0, 0.0]\n_, yCompass = eval_compass_difference(f, x)\n    \n# output \n    \nyCompass = [0.5, 0.0, -0.5]\n\n\n\n\n\n","category":"function"},{"location":"convexOptimization/functions.html#Exported-Functions","page":"Exported Functions","title":"Exported Functions","text":"","category":"section"},{"location":"convexOptimization/functions.html","page":"Exported Functions","title":"Exported Functions","text":"Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton\r\nMain.NonsmoothFwdAD.ConvexOptimization.LPNewton\r\nMain.NonsmoothFwdAD.ConvexOptimization.levelMethod","category":"page"},{"location":"convexOptimization/functions.html#Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton","page":"Exported Functions","title":"Main.NonsmoothFwdAD.ConvexOptimization.semiSmoothNewton","text":"semiSmoothNewton(f::Function, x0::Vector{Float64}, kwargs…)\n\nCompute vector x where f(x) = 0 using the semi-smooth Newton method where there are multiple inputs to f but only one output. \n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot.\nx0::Vector{Float64}: initial domain vector guess \n\nKeywords\n\nepsilon::Float64: tolerance for solver stopping condition where epsilon + delta*norm(f(x0)). Set to 0.05 by default. \ndelta::Float64: tolerance for solver stopping condition where epsilon + delta*norm(f(x0)). Set to 0.00005 by default. \nmaxIter::Int64: maximum number of solver iterations. Set to 1000 by default. \n\n\n\n\n\n","category":"function"},{"location":"convexOptimization/functions.html#Main.NonsmoothFwdAD.ConvexOptimization.LPNewton","page":"Exported Functions","title":"Main.NonsmoothFwdAD.ConvexOptimization.LPNewton","text":"LPNewton(f::Function, z0::Vector{Float64}, kwargs…)\n\nCompute vector z where f(z) = 0 using the Linear Program Newton method where the number of inputs to f is equivalent to number of outputs. \n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot.\nx0::Vector{Float64}: initial domain vector guess \n\nKeywords\n\nConstant and tolerance keyword\n\nepsilon::Float64: tolerance for solver stopping condition where f(z) >= epsilon. Set to 1e-2 by default. \nsolverTolerance::Float64: tolerance for JuMP solver. Set to 1e-6 by default.\nmaxIter::Int64: maximum number of solver iterations. Set to 20 by default. \n\nConstraint keywords\n\nlb::Vector{Float64}: lower bound on domain vector x. Set to -Inf vector of size n by default.\nub::Vector{Float64}: upper bound on domain vector x. Set to -Inf vector of size n by default.\nA::Matrix{Float64}: matrix A in inequality constraint set A*z <= b. Set to zeros vector of size n by default.\nb::Vector{Float64}: matrix b in inequality constraint set A*z <= b. Set to zeros matrix of size 1x1.\nAeq::Matrix{Float64}: matrix A in equality constraint set A*z == b. Set to zeros vector of size n by default. \nbeq::Vector{Float64}: matrix b in equality constraint set A*z == b. Set to zeros matrix of size 1x1.\n\nwhere n is the length of vector z0.\n\n\n\n\n\n","category":"function"},{"location":"convexOptimization/functions.html#Main.NonsmoothFwdAD.ConvexOptimization.levelMethod","page":"Exported Functions","title":"Main.NonsmoothFwdAD.ConvexOptimization.levelMethod","text":"levelMethod(f::Function, z0::Vector{Float64}, kwargs…)\n\nCompute vector x to minimize f(x) using the Level method where there are multiple inputs to f but only one output. . \n\nArguments\n\nf::Function: must be continous and of finite compositions of elemental operations. Each operation must be of the form f(x::N)::Float64 where N is either Vector{Float64} or Float64 ; otherwise implementation cannot map f. Supported elemental operations include: +, -, *, inv, /, ^, exp, log, sin, cos, abs, min, max, hypot.\nx0::Vector{Float64}: initial domain vector guess \n\nKeywords\n\nConstant and tolerance keywords\n\nepsilon::Float64: tolerance for solver stopping condition where f(z) >= epsilon. Set to 1e-2 by default. \nsolverTolerance::Float64: tolerance for JuMP solver. Set to 1e-6 by default.\nalpha::Float64: level set constant for quadratic programming. Set to 1/(2+sqrt(2) by default. \nmaxIter::Int64: maximum number of solver iterations. Set to 20 by default. \n\nConstraint keywords\n\nlb::Vector{Float64}: lower bound on domain vector x. Set to -Inf vector of size n by default.\nub::Vector{Float64}: upper bound on domain vector x. Set to -Inf vector of size n by default.\nA::Matrix{Float64}: matrix A in inequality constraint set A*z <= b. Set to zeros vector of size n by default.\nb::Vector{Float64}: matrix b in inequality constraint set A*z <= b. Set to zeros matrix of size 1x1.\nAeq::Matrix{Float64}: matrix A in equality constraint set A*z == b. Set to zeros vector of size n by default. \nbeq::Vector{Float64}: matrix b in equality constraint set A*z == b. Set to zeros matrix of size 1x1.\n\nwhere n is the length of vector z0.\n\n\n\n\n\n","category":"function"}]
}
